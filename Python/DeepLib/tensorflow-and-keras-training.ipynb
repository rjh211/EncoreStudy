{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n\nimport tensorflow.compat.v1 as tf\n\nfrom keras import layers\nfrom keras import models\nfrom keras import optimizers\nfrom keras import preprocessing\nfrom keras.applications import ResNet101\nfrom keras.datasets import mnist\nfrom keras.datasets import imdb\nfrom keras.layers import Flatten, Dense, Embedding\nfrom keras.layers import SimpleRNN, LSTM\nfrom keras.models import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\n\nfrom konlpy.tag import Twitter\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\nimport math, sys\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n#tf.compat.v1.disable_eager_execution()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.compat.v1.Session() as ses:\n    # Build a graph.\n    a = tf.constant(5.0)\n    b = tf.constant(6.0)\n    c = a * b\n\n    # Evaluate the tensor `c`.\n    print(ses.run(c))\n    \n    a = tf.constant(120,name = \"a\")  #상수 생성\n    b = tf.constant(130,name = \"b\")\n    c = tf.constant(140,name = \"c\")\n    v = tf.Variable(0, name = \"v\")   #변수 생성\n\n    calc_op = a + b + c                  #수식 입력\n    assign_op = tf.assign(v, calc_op)#계싼 결과와 변수연결\n\n    sess = tf.Session()              #세션 수립, 시작\n    sess.run(assign_op)              #연산 실행\n\n    print(sess.run(v))               #v에 담긴값 출력\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.compat.v1.Session() as ses:    \n    a = tf.placeholder(tf.int32, [3])\n    print(a)\n    b = tf.constant(2)\n    print(b)\n    x2_op = a * b\n    \n    r1 = ses.run(x2_op, feed_dict = {a:[1,2,3]})\n    print(r1)\n    \n    r2 = ses.run(x2_op, feed_dict = {a:[10,20,30]})\n    print(r2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nxor_data = [[0,0],[0,1],[1,0],[1,1]]\nxor_label = [[0],[1],[1],[0]]\n\nx = tf.placeholder(tf.float32, [None,2])#데이터\ny = tf.placeholder(tf.float32, [None,1])#라벨\n\nw = tf.Variable(tf.zeros([2,1]))#가중치 초기화\nb = tf.Variable(tf.zeros([1])) # 바이어스 초기화\n\na = tf.nn.softmax(tf.matmul(x, w) + b) #소프트맥스 회귀함수 정의\ncross_entropy = -tf.reduce_sum(a * tf.log(a)) # 크로스엔트로피 정의\noptimizer = tf.train.GradientDescentOptimizer(0.01) #옵티마이저 정의\ntrain = optimizer.minimize(cross_entropy) #학습 시작\n\npredict = tf.equal(tf.argmax(a,1), tf.argmax(y,1)) # 정답률 계산 수식\naccuracy = tf.reduce_mean(tf.cast(predict, tf.float32))\n\nsess = tf.Session()  # 실행\nsess.run(tf.global_variables_initializer())\nfd = {x: xor_data, y : xor_label}\n\nfor i in range(10000):\n    sess.run(train, feed_dict=fd)#훈련\n    if i % 500 == 0:\n        cre = sess.run(cross_entropy, feed_dict=fd)#w, b 재조정\n        acc = sess.run(accuracy, feed_dict={x: xor_data, y: xor_label})\n        print(i, \"번 수행. 정답률 =\", acc)\nacc = sess.run(accuracy, feed_dict={x: xor_data, y: xor_label})#테스팅\nprint(\"최종 정답률 =\", acc)\npre = sess.run(tf.argmax(a, 1), feed_dict={x: [[1,0]]})#학습 확인\nprint(pre)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.compat.v1.disable_eager_execution()\ncsv = pd.read_csv(\"../input/bmicsv/bmi.csv\")\n\ncsv[\"height\"] = csv[\"height\"] / 200\ncsv[\"weight\"] = csv[\"weight\"] / 100\n#one-hot 레이블\nbclass = {\"thin\": [1,0,0], \"normal\": [0,1,0], \"fat\": [0,0,1]}\ncsv[\"label_pat\"] = csv[\"label\"].apply(lambda x : np.array(bclass[x]))\n\ntest_csv = csv[15000:20000]\ntest_pat = test_csv[[\"weight\",\"height\"]]\ntest_ans = list(test_csv[\"label_pat\"])\n\nx  = tf.placeholder(tf.float32, [None, 2]) \ny_ = tf.placeholder(tf.float32, [None, 3]) \n\nW = tf.Variable(tf.zeros([2, 3])); #가중치\nb = tf.Variable(tf.zeros([3])); #바이어스 \n#y = wx+b. 실제 값과 예측값의 분산을 계산할 회귀함수 지정\ny = tf.nn.softmax(tf.matmul(x, W) + b)\n\n#손실을 최소화하기 위한 최적화\ncross_entropy = -tf.reduce_sum(y_ * tf.log(y))\noptimizer = tf.train.GradientDescentOptimizer(0.01)\ntrain = optimizer.minimize(cross_entropy)\n\n#정답\npredict = tf.equal(tf.argmax(y, 1), tf.argmax(y_,1))\n#정답률\naccuracy = tf.reduce_mean(tf.cast(predict, tf.float32))\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer()) # 변수 초기화하기\n\nfor step in range(3500):\n    i = (step * 100) % 14000\n    rows = csv[1 + i : 1 + i + 100]#무작위 위치의 100줄 추출\n    x_pat = rows[[\"weight\",\"height\"]]\n    y_ans = list(rows[\"label_pat\"])\n    fd = {x: x_pat, y_: y_ans}\n    sess.run(train, feed_dict=fd)#학습\n    #테스팅\n    if step % 500 == 0:\n        cre = sess.run(cross_entropy, feed_dict=fd)\n        acc = sess.run(accuracy, feed_dict={x: test_pat, y_: test_ans})\n        print(\"step=\", step, \"cre=\", cre, \"acc=\", acc)\n\nacc = sess.run(accuracy, feed_dict={x: test_pat, y_: test_ans})\nprint(\"정답률 =\", acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = models.Sequential()\n\nmodel.add(layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (28,28,1)))\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(64, (3,3),activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(64, (3,3), activation = 'relu'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation = 'relu'))\nmodel.add(layers.Dense(10, activation = 'softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\ntrain_images = train_images.reshape((60000,28,28,1))\ntrain_images = train_images.astype('float32') / 255\n\ntest_images = test_images.reshape((10000, 28, 28, 1))\ntest_images - test_images.astype('float32') /  255\n\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\nmodel.fit(train_images, train_labels, epochs = 5, batch_size = 64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_loss, test_acc = model.evaluate(test_images, test_labels)\ntest_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nprint(test_images.shape)\npre = model.predict(test_images)\nprint(test_labels[np.argmax(pre[4])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3,3), activation = 'relu', input_shape = (150,150,3)))\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(64, (3,3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(128, (3,3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(128, (3,3), activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation = 'relu'))\nmodel.add(layers.Dense(1, activation = 'sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"conv2D : convolution layer\nparam1 : filter's dim.\nparam2 : filterSize, stribe, padding(zero padding = 'same', else = 'valid')\n\nMaxPooling2D : featureMap's size down\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer = optimizers.RMSprop(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\noriginal_dataset_dir = '../input/dataset/datasets/cats_and_dogs/train/'\nbase_dir = '../input/dataset/datasets/cats_and_dogs_small/'\nvalidation_dir = base_dir +'validation/'\ntest_dir = base_dir + 'test/'\ntrain_dir = base_dir + 'train/'\ntrain_cats_dir = train_dir +'cats/'\ntrain_dogs_dir = train_dir + 'dogs/'\n\nvaildation_cats_dir = validation_dir + 'cats/'\nvaildation_dogs_dir = validation_dir + 'dogs/'\n\ntest_cats_dir = test_dir + 'cats'\ntest_dogs_dir = test_dir + 'dogs'\n\ntrain_datagen = ImageDataGenerator(rescale = 1./255)\ntest_datagen = ImageDataGenerator(rescale = 1./255)\n\ntrain_generator = train_datagen.flow_from_directory(train_dir, target_size = (150,150), batch_size = 20, class_mode = 'binary')   #class_model => 결과의 개수에 따라 binary, classify 로나뉨\nvalidation_generator = test_datagen.flow_from_directory(validation_dir, target_size = (150,150), batch_size = 20, class_mode = 'binary')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"from keras.preprocessing import image\n\nimg_path = 'cat1.jpg'\nimg = image.load_img(img_path, target_size=(150,150,))\n\nx = image.img_to_array(img)\nx = x.reshape((1,) + x.shape)\ni = 0 \nfor batch in datagen.flow(x, batch_size = 1):\n    plt.figure(i)\n    imgplot = plt.imshow(image.array_to_img(batch[0]))\n    i+= 1\n    if i % 4 == 0: break\n    \n  plt.show()","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for data_batch, labels_batch in train_generator:\n    print('배치 데이터 크기 : ', data_batch.shape)\n    print('라벨 데이터 크기 : ', labels_batch.shape)\n    break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(train_generator, steps_per_epoch = 100, epochs = 30, validation_data = validation_generator, validation_steps = 50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('../working/cats_and_dogs_small_1.h5')  # 학습 결과저장","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\nmodel = load_model('../working/cats_and_dogs_small_1.h5')\nvalidation_generator.class_indices\nmodel.predict_generator(validation_generator, steps = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label = 'Training acc')\nplt.plot(epochs, val_acc , 'b', label = 'Validation acc')\nplt.title('Trainig and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss , 'bo', label = 'Training loss')\nplt.plot(epochs, val_loss, 'b', label = 'Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications import VGG16\nconv_base = VGG16(weights = 'imagenet', include_top = False, input_shape=(150,150,3))\nconv_base.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nconv_base1 = ResNet101(weights = 'imagenet', include_top = False, input_shape=(224,224,3))\nconv_base1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsamples = ['The cat sat on the mat,','The dog ate my homework']\ntoken_index = {}\nfor sample in samples:\n    for word in sample.split():\n        if word not in token_index:\n            token_index[word] = len(token_index) + 1\nmax_length = 10\nresults = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\nfor i , sample in enumerate(samples):                                                   #문장의번호 \n    for j , word in list(enumerate(sample.split()))[:max_length]:            #문장내에서 단어의 위치\n        index = token_index.get(word)\n        results[i,j,index] = 1\nprint(token_index)\nprint(results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"one hot encoding 구현","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsamples = ['The cat sat on the mat,','The dog ate my homework']\ntokenizer = Tokenizer(num_words = 10)\ntokenizer.fit_on_texts(samples)\nsequences = tokenizer.texts_to_sequences(samples)\none_hot_results = tokenizer.texts_to_matrix(samples, mode = 'binary')\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' %len(word_index))\nprint(word_index)\nprint(sequences)\nprint(one_hot_results)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"one hot encoding using keras API","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nmax_features = 10000\nmaxlen = 20\n\n(x_train, y_train),(x_test, y_test) = imdb.load_data(num_words = max_features)\nx_train = preprocessing.sequence.pad_sequences(x_train, maxlen = maxlen)\nx_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"IMDB Data Set을 이용하여 문자열 처리 (영화 리뷰를 통해 영화 평가가 긍정인지 부정인지 확인)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nmodel = Sequential()\nmodel.add(Embedding(10000,8,input_length = maxlen))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(optimizer = 'rmsprop',loss = 'binary_crossentropy', metrics = ['acc'])\nmodel.summary()\n\nhistory = model.fit(x_train, y_train, epochs = 10, batch_size = 32, validation_split = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre = model .predict(x_test)\npre","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label = 'Training acc')\nplt.plot(epochs, val_acc , 'b', label = 'Validation acc')\nplt.title('Trainig and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss , 'bo', label = 'Training loss')\nplt.plot(epochs, val_loss, 'b', label = 'Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imdb_dir = '../input/imdbkeras/aclImdb/aclImdb/'\n\ntrain_dir = os.path.join(imdb_dir, 'train')\nlabels = []\ntexts = []\n\nfor label_type in['neg', 'pos']:\n    dir_name = os.path.join(train_dir, label_type)\n    for fname in os.listdir(dir_name):\n        if fname[-4:] == '.txt':\n            f = open(os.path.join(dir_name, fname), encoding = 'utf8')\n            texts.append(f.read())\n            f.close()\n            if label_type == 'neg':\n                labels.append(0)\n            else:\n                labels.append(1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Embedding 방식을 사용하여 각 단어간 관계를 파악하여 결과 도출","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nmaxlen = 100\ntraining_samples = 10000\nvalidation_samples = 10000\nmax_words = 10000\n\ntokenizer = Tokenizer(num_words = max_words)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\n\nword_index = tokenizer.word_index\nprint('%s개의 고유한 토큰을 찾았습니다.' %len(word_index))\n\ndata = pad_sequences(sequences, maxlen=maxlen)    #빈공간은 0으로 채움\n\nlabels = np.asarray(labels)                       #List to np\nprint('데이터 텐서의 크기:', data.shape)\nprint('라벨의 크기:', labels.shape)\n\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\n\nx_train = data[:training_samples]\ny_train = labels[:training_samples]\nx_val = data[training_samples : training_samples + validation_samples]\ny_val = labels[training_samples : training_samples + validation_samples]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove_dir = '../input/imdbkeras/'\n\nembeddings_index = {}\nf = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding = 'utf8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype = 'float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('%s개의 단어 벡터를 찾았습니다.'%len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 100\nembedding_matrix= np.zeros((max_words, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if i<max_words:\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length = maxlen))\nmodel.add(Flatten())\nmodel.add(Dense(32, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\nhistory = model.fit(x_train, y_train, epochs = 10, batch_size = 32, validation_data = (x_val, y_val))\nmodel.save_weights('../working/pre_trainded_glove_model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = x_val[:10]\ny_test = y_val[:10]\nprint(y_test)\npre = model.predict(x_test)\nprint(pre)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label = 'Training acc')\nplt.plot(epochs, val_acc , 'b', label = 'Validation acc')\nplt.title('Trainig and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss , 'bo', label = 'Training loss')\nplt.plot(epochs, val_loss, 'b', label = 'Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Embedding 방식보다 그냥 one hot encoding 방식만 적용했을 경우 정확도가 더 높다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.datasets import reuters\n(train_data, train_label), (test_Data, test_labels) = reuters.load_data(num_words = 10000)\nword_index = reuters.get_word_index()\nreverse_word_index = idct([(value,key)for (key,value) in word_index.items()])\ndecoded_newswire = ' '.join([reverse_word_index.get(i-3,'?') for i in train_data[0]])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = reuters.get_word_index()\nreverse_word_index = idct([(value,key)for (key,value) in word_index.items()])\ndecoded_newswire = ' '.join([reverse_word_index.get(i-3,'?') for i in train_data[0]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def vectorize_sequences(sequences, dimentsion):\n    results = np.zeros((len(sequence)))\n    for i , sequence in enumerate(sequences):\n        result[i,sequence] =1,\n    \n    x_test = vectorsize =  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir_path = '../input/furniture/img/train/'\nvalidation_dir_path = '../input/furniture/img/val/'\nvalidation_dir_path = '../input/furniture/img/val/'\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"firniture train and validation directory path","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = models.Sequential()\nmodel.add(layers.Conv2D(32,(3, 3), activation = 'relu', input_shape = (150,150,3)))\n\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(64,(3,3),activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(128,(3,3),activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(128,(3,3),activation = 'relu'))\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation = 'relu'))\nmodel.add(layers.Dense(5, activation = 'softmax'))\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"model 구축","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer = optimizers.RMSprop(lr = 1e-4), loss = 'categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_datagen = ImageDataGenerator(rescale = 1./255)\ntest_datagen = ImageDataGenerator(rescale = 1./255)\ntrain_generator = train_datagen.flow_from_directory(train_dir_path, target_size = (150,150), batch_size = 20, class_mode = 'categorical')\nvalidation_generator = test_datagen.flow_from_directory(validation_dir_path, target_size = (150,150), batch_size = 20, class_mode = 'categorical')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for data_batch, labels_batch in train_generator:\n    print('배치 데이터 크기 : ', data_batch.shape)\n    print('라벨 데이터 크기 : ', labels_batch.shape)\n    break\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(train_generator, steps_per_epoch = 40, epochs = 30, validation_data = validation_generator, validation_steps = 5)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('../working/furniture_detect.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import load_model\nmodel = load_model('../working/furniture_detect.h5')\nvalidation_generator.class_indices\nmodel.predict_generator(validation_generator, steps = 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label = 'Training acc')\nplt.plot(epochs, val_acc , 'b', label = 'Validation acc')\nplt.title('Trainig and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss , 'bo', label = 'Training loss')\nplt.plot(epochs, val_loss, 'b', label = 'Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pre = model.predict_generator(validation_generator, steps = 1)\nfor p in pre :\n    print(np.argmax(p))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(num):\n    plt.figure(i)\n    imgplot = plt.imshow(validation_generator[0][0][i])\n    x = validation_generator[0][0][i]\n    x = x.reshape(1,150,150,3)\n    pre = model.predict(x)\n    print('예측 : ',np.argmax(pre), end = ' / ')\n    print('정답 : ',validation_generator[0][1][i])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"validation data를 사용하여 평가","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(10000,32))\nmodel.add(SimpleRNN(32, return_sequences = True))\nmodel.add(SimpleRNN(32, return_sequences = True))\nmodel.add(SimpleRNN(32, return_sequences = True))\nmodel.add(SimpleRNN(32, return_sequences = False))  #default = False\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"RNN(using SimpleRNN)의 기본 구조\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 10000\nmaxlen = 500\nbatch_size = 32\n\nprint('data loading..')\n(input_train, y_train),(input_test, y_test) = imdb.load_data(num_words = max_features)\nprint(len(input_train), '훈련 시퀀스')\nprint(len(input_test), '테스트 시퀀스')\n\nprint('시퀀스 패딩 (samples x time)')\ninput_train = pad_sequences(input_train, maxlen = maxlen)\ninput_test = pad_sequences(input_test, maxlen = maxlen)\n\nprint('input_train 크기 : ', input_train.shape)\nprint('input_test 크기 : ', input_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(SimpleRNN(32, return_sequences = True))\nmodel.add(SimpleRNN(32, return_sequences = False))  #default = False\nmodel.add(Dense(1, activation = 'tanh'))\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics=['acc'])\nhistory = model.fit(input_train, y_train,epochs = 10, batch_size = 128, validation_split = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('../working/rnnTest.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label = 'Training acc')\nplt.plot(epochs, val_acc , 'b', label = 'Validation acc')\nplt.title('Trainig and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss , 'bo', label = 'Training loss')\nplt.plot(epochs, val_loss, 'b', label = 'Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(LSTM(32, return_sequences = False))\nmodel.add(Dense(1, activation = 'tanh'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics= ['acc'])\nhistory = model.fit(input_train, y_train, epochs = 10, batch_size = 128, validation_split = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('../working/LSTMTest.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label = 'Training acc')\nplt.plot(epochs, val_acc , 'b', label = 'Validation acc')\nplt.title('Trainig and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss , 'bo', label = 'Training loss')\nplt.plot(epochs, val_loss, 'b', label = 'Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(LSTM(32, return_sequences = True))\nmodel.add(LSTM(32, return_sequences = False))\nmodel.add(Dense(1, activation = 'tanh'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics= ['acc'])\nhistory = model.fit(input_train, y_train, epochs = 10, batch_size = 128, validation_split = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('../working/doubleLSTMTest.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label = 'Training acc')\nplt.plot(epochs, val_acc , 'b', label = 'Validation acc')\nplt.title('Trainig and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss , 'bo', label = 'Training loss')\nplt.plot(epochs, val_loss, 'b', label = 'Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nclass BayesianFilter:\n    \"\"\" 베이지안 필터 \"\"\"\n    def __init__(self):\n        self.words = set() # 출현한 단어 기록\n        self.word_dict = {} # 카테고리마다의 출현 횟수 기록\n        self.category_dict = {} # 카테고리 출현 횟수 기록\n    # 형태소 분석하기 --- (※1)\n    def split(self, text):\n        results = []\n        twitter = Twitter()\n        # 단어의 기본형 사용\n        malist = twitter.pos(text, norm=True, stem=True)\n        for word in malist:\n            # 어미/조사/구두점 등은 대상에서 제외 \n            if not word[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n                results.append(word[0])\n        return results\n    # 단어와 카테고리의 출현 횟수 세기 --- (※2)\n    def inc_word(self, word, category):\n        # 단어를 카테고리에 추가하기\n        if not category in self.word_dict:\n            self.word_dict[category] = {}\n        if not word in self.word_dict[category]:\n            self.word_dict[category][word] = 0\n        self.word_dict[category][word] += 1\n        self.words.add(word)\n    def inc_category(self, category):\n        # 카테고리 계산하기\n        if not category in self.category_dict:\n            self.category_dict[category] = 0\n        self.category_dict[category] += 1\n    \n    # 텍스트 학습하기 --- (※3)\n    def fit(self, text, category):\n        \"\"\" 텍스트 학습 \"\"\"\n        word_list = self.split(text)\n        for word in word_list:\n            self.inc_word(word, category)\n        self.inc_category(category)\n    \n    # 단어 리스트에 점수 매기기--- (※4)\n    def score(self, words, category):\n        score = math.log(self.category_prob(category))\n        for word in words:\n            score += math.log(self.word_prob(word, category))\n        return score\n    \n    # 예측하기 --- (※5)\n    def predict(self, text):\n        best_category = None\n        max_score = -sys.maxsize \n        words = self.split(text)\n        score_list = []\n        for category in self.category_dict.keys():\n            score = self.score(words, category)\n            score_list.append((category, score))\n            if score > max_score:\n                max_score = score\n                best_category = category\n        return best_category, score_list\n    # 카테고리 내부의 단어 출현 횟수 구하기\n    def get_word_count(self, word, category):\n        if word in self.word_dict[category]:\n            return self.word_dict[category][word]\n        else:\n            return 0\n    # 카테고리 계산\n    def category_prob(self, category):\n        sum_categories = sum(self.category_dict.values())\n        category_v = self.category_dict[category]\n        return category_v / sum_categories\n        \n    # 카테고리 내부의 단어 출현 비율 계산 --- (※6)\n    def word_prob(self, word, category):\n        n = self.get_word_count(word, category) + 1 # ---(※6a)\n        d = sum(self.word_dict[category].values()) + len(self.words)\n        return n / d","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}